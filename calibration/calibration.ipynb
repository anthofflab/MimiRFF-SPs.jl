{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "\n",
    "This file will describe sources for the data in this repository, as well as any extra processing information.  \n",
    "\n",
    "Two zipped datasets are available online here [TODO], `calibration_raw_data` and `RFFSPs_large_datafiles`.  The latter is automatically downloaded with this package upon first use using the `DataDeps` julia package, so you should accept the request to download these data and don't need to do anything further to obtain what is needed to run the component.\n",
    "\n",
    "To understand any post-processing done from raw files to those in `RFFSPs_large_datafiles`, you may download `calibration_raw_data` to the current `calibration` folder, and run the scripts below, which will do any post-processing we did as a group and put files into their respective locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions\n",
    "\n",
    "The processes for the emissions data can be found in the [RFF Socioeconomic Projections repository](https://github.com/rffscghg/rff-socioeconomic-projections) and downloaded from [TODO] into `data/calibration`.  These inputs files include;\n",
    "\n",
    "- CH4_Emissions_Trajectories.csv\n",
    "- N2O_Emissions_Trajectories.csv\n",
    "- CO2_Emissions_Trajectories.csv\n",
    "\n",
    "We postprocess these files as follows to (1) long format (2) the same time dimension available for socioeconomics and save them to `RFFSPs_large_datafiles/emissions` using the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSVFiles, Query\n",
    "\n",
    "outdir = joinpath(@__DIR__, \"..\", \"data\", \"RFFSPs_large_datafiles\", \"emissions\")\n",
    "isdir(outdir) || mkpath(outdir)\n",
    "\n",
    "files = [\"CH4_Emissions_Trajectories.csv\", \"N2O_Emissions_Trajectories.csv\", \"CO2_Emissions_Trajectories.csv\"]\n",
    "for file in files\n",
    "    df = load(joinpath(@__DIR__, \"calibration_raw_data\", file)) |> \n",
    "        DataFrame |>\n",
    "        i -> stack(i, Not(:sample)) |>\n",
    "        DataFrame |>\n",
    "        i -> rename!(i, [:sample, :year, :value]) |>\n",
    "        DataFrame\n",
    "    \n",
    "    df.year = parse.(Int64, df.year)\n",
    "\n",
    "    df |> @filter(_.year in collect(2020:2300)) |> \n",
    "        DataFrame |>\n",
    "        save(joinpath(outdir, file))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socioeconomic\n",
    "\n",
    "The processes for the socioeconomics data can be found in the [RFF Socioeconomic Projections repository](https://github.com/rffscghg/rff-socioeconomic-projections) and downloaded from [TODO] and saved in `RFFSPs_large_datafiles/rffsps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Mortality\n",
    "\n",
    "In order to run some damage functions, users need baseline mortality data harmonized to a given population trajectory.  Baseline mortality data found in `data/mortality` was derived from the `death_rates.csv` provided to the RFF team on October 7th from Hana Sevcikova and available (privately) here: https://drive.google.com/open?id=1TCYgzRJyt-8wadRcCfxDdpqF84t6oZfj&authuser=hanas%40uw.edu&usp=drive_fs to be downloaded to `calibration/data`. Column `DeathRate` is the annual deaths per 1000 people. `PopAvg` is the denominator (average between two time periods) and `PopStart` is population at the start of the time interval. Values are average deaths per 1000 people. \n",
    "\n",
    "There is one mortality death rate trajectory for each population trajectory, and each RFF SP is matched to one of these 1000 trajectories. The key file for this matching is in `data/keys/sampled_pop_trajectory_numbers.csv`.\n",
    "\n",
    "Post processing of `death_rates.csv` into `death_rates/death_rates_TrajectoryX.csv` is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file _sampled_pop_trajectory_numbers.csv_ maps each of the 10,000 RFF SP scenarios to the baseline mortality scenario (out of 1000) matched to its population draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, Query, CSVFiles, Arrow, CategoricalArrays\n",
    "\n",
    "outdir = joinpath(@__DIR__, \"..\", \"data\", \"RFFSPs_large_datafiles\", \"death_rates\")\n",
    "isdir(outdir) || mkpath(outdir)\n",
    "\n",
    "# load the ISO3 codes we want to use\n",
    "countries = load(joinpath(@__DIR__, \"..\", \"data\", \"keys\", \"MimiRFFSPs_ISO3.csv\")) |> DataFrame \n",
    "\n",
    "# process death_rates.csv into 1000 trajectories\n",
    "df = load(joinpath(@__DIR__, \"calibration_raw_data\", \"death_rates.csv\")) |> \n",
    "    DataFrame |> \n",
    "    @filter(_.LocID in countries.NumericCode) |>\n",
    "    @select(:LocID, :Year, :Trajectory, :DeathRate) |> \n",
    "    DataFrame\n",
    "\n",
    "# get ISO3 codes\n",
    "idxs = indexin(df.LocID, countries.NumericCode)\n",
    "insertcols!(df, 1, :ISO3 => countries.ISO3[idxs])\n",
    "select!(df, Not(:LocID))\n",
    "\n",
    "for t in unique(df.Trajectory)\n",
    "    \n",
    "    filtered_df = df |> \n",
    "        @filter(_.Trajectory == t) |> \n",
    "        DataFrame |> \n",
    "        i -> select!(i, Not(:Trajectory)) |> \n",
    "        DataFrame\n",
    "\n",
    "    # interpolate the years\n",
    "    trajectory_df = DataFrame(\n",
    "        :ISO3 => reduce(vcat, [fill(i, 5) for i in filtered_df.ISO3]),\n",
    "        :Year => reduce(vcat, [collect(2021:2300) for i in 1:length(unique(filtered_df.ISO3))]),\n",
    "        :DeathRate => reduce(vcat, [fill(i, 5) for i in filtered_df.DeathRate])\n",
    "    )\n",
    "\n",
    "    start_rows = deepcopy(trajectory_df |> @filter(_.Year == 2021) |> DataFrame) # get 2021 rows\n",
    "    start_rows.Year .= 2020 # use 2020\n",
    "    append!(trajectory_df, start_rows)\n",
    "    sort!(trajectory_df, [:ISO3, :Year])\n",
    "\n",
    "    # some compression and types\n",
    "    trajectory_df.Year = convert.(Int16, trajectory_df.Year)\n",
    "    trajectory_df.DeathRate = convert.(Float64, trajectory_df.DeathRate)\n",
    "    trajectory_df.ISO3 = categorical(trajectory_df.ISO3, compress = true)\n",
    "\n",
    "    trajectory_df |> Arrow.write(joinpath(outdir, \"death_rates_Trajectory$t.feather\"))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
